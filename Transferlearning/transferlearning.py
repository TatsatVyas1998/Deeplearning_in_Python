# -*- coding: utf-8 -*-
"""transferlearning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16QIqjUoZGcB3HnynkAkPzXPeWwp6A_uj
"""

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy

plt.ion()

!unzip /Linnaeus5new.zip
#

# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = '/content/Linnaeus'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'test']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=50,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'test']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

print(image_datasets)

"""This loading of the data is taken from the pytorch tutorial. The purpose of the above code is to simply just preprocess the data and load it into a dataloader"""

network = torchvision.models.vgg16(pretrained=True)
for param in network.parameters():  #freezing the feature extraction layers
    param.requires_grad = False

#now we change classification layers with the number of outputs equal to the numbers of classes in our dataset.
network.classifier =  nn.Sequential(
   nn.BatchNorm1d(25088),
    #nn.Dropout(p=0.3),
    nn.Linear(in_features=25088, out_features=4096 ,bias= False),
    nn.ReLU(inplace=True),
    nn.Dropout(p=0.5, inplace=False ),
    nn.Linear(in_features=4096, out_features=4096 , bias = False),
    nn.ReLU(inplace=True),
    nn.Dropout(p=0.5, inplace=False ),
    nn.Linear(in_features=4096, out_features=5 , bias = False),
)
network.train()

for name, param in network.named_parameters(): #checking
    
    if param.requires_grad:
      print (name)

import torch.nn.functional as F
import torch.optim as optim
from torch.optim import lr_scheduler

device = torch.device('cuda') #using GPU to make the training faster
print(device)
#torch.device('cuda')
network = network.to(device)

def get_num_correct(preds, labels):
    return preds.argmax(dim=1).eq(labels).sum().item()

optimizer = optim.Adam( network.classifier.parameters() , lr = 0.01)
total_loss = 0
total_correct = 0
#this gives us acsess to the data and queting capabilities
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
train_loader = dataloaders['train']
for epoch in range(10): #10 epochs
    total_loss = 0
    total_correct = 0
    for b in train_loader:  #600 batches and each batch with length of 100 so 600000 which is our dataset
        
        images , labels =  b[0].to(device,dtype=torch.float) , b[1].to(device)
        preds = network(images) 
        loss = F.cross_entropy(preds , labels) #calculating the loss fucntion
        optimizer.zero_grad() #pytoch accumilates the gradient so we need to zero out any values that is currently in the gradient
        loss.backward() #back propogation and calculating the gradient
        optimizer.step() #updating the weights

        total_loss += loss.item()
        total_correct += get_num_correct(preds, labels)
    exp_lr_scheduler.step()
    print( "epoch", epoch, "Accuracy" , (total_correct/dataset_sizes['train'])*100, "total loss" ,  total_loss)

print(len(train_loader.dataset))

L = []
network = network.eval()
network.requires_grad_(False); #turning all the gradients off because we are testing.

def get_all_preds( model , loader):
    total_loss = 0
    total_correct = 0
    all_preds = torch.tensor([])
    for b in loader:

        images , labels = b[0].to(device) , b[1].to(device)
        preds = model(images)
        preds = preds.to(device)
        all_preds= torch.cat( (all_preds.to(device), preds), dim=0)
        torch.cuda.reset_max_memory_allocated()
        total_correct += get_num_correct(preds, labels)
        L.append(labels)
    return all_preds , total_correct

predicstion_loader = dataloaders['test']
train_preds , total_correct  = get_all_preds( network , predicstion_loader)  
print(total_correct)
print(image_datasets['test'].classes)
print( 'accuracy test', total_correct/len(train_preds)*100)

len(predicstion_loader.dataset)



k = torch.stack(L)

L = k.reshape(-1)
print(train_preds)

from sklearn import metrics
train_preds = train_preds.argmax(dim=1)
confusion_matrix = metrics.confusion_matrix( train_preds.to('cpu') , L.to('cpu'))

print(confusion_matrix)

num_classes = 10
class_names = ['berry', 'bird', 'dog', 'flower', 'other']                

plt.figure()
plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)

tick_marks = np.arange(num_classes)
classNames = class_names

thresh = confusion_matrix.max() / 2.
for i in range(confusion_matrix.shape[0]):
    for j in range(confusion_matrix.shape[1]):
        plt.text(j, i, format(confusion_matrix[i, j]),
                ha="center", va="center",
                color="white" if  confusion_matrix[i, j] == 0 or confusion_matrix[i, j] > thresh else "black") 
plt.tight_layout()
plt.colorbar()
plt.xlabel(classNames)
plt.show()

pres , recall , fbeta_score , support = metrics.precision_recall_fscore_support(train_preds.to('cpu') , L.to('cpu') )

print(pres)

print(recall)

print(fbeta_score)

m = confusion_matrix.diagonal
TP = np.diag(confusion_matrix)
#print(TP)
FP = np.sum(confusion_matrix, axis=0) - TP
#print(FP)
FN = np.sum(confusion_matrix, axis=1) - TP
#print(FN)

precision = TP/(TP+FP)
recall = TP/(TP+FN)
print("precision : " , precision)
print("recall : " , recall)
fscore = (2* precision * recall) /(precision + recall)
print("fscore : " , fscore)
print( 'accuracy test', total_correct/len(train_preds)*100)